{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3debd806",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sayfp\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing split dataset. Loading splits...\n",
      "Calculating class weights for balanced sampling...\n",
      "Data loaders are ready.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sayfp\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sayfp\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ConvNeXt_Base_Weights.IMAGENET1K_V1`. You can also use `weights=ConvNeXt_Base_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Error while downloading from https://cdn-lfs.hf.co/repos/fb/cd/fbcdc88e492959e3ee2515f497fb45cb5f217f9455c204bdf4e0b400c90d0c23/32aa17d6e17b43500f531d5f6dc9bc93e56ed8841b8a75682e1bb295d722405b?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model.safetensors%3B+filename%3D%22model.safetensors%22%3B&Expires=1753078349&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MzA3ODM0OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy9mYi9jZC9mYmNkYzg4ZTQ5Mjk1OWUzZWUyNTE1ZjQ5N2ZiNDVjYjVmMjE3Zjk0NTVjMjA0YmRmNGUwYjQwMGM5MGQwYzIzLzMyYWExN2Q2ZTE3YjQzNTAwZjUzMWQ1ZjZkYzliYzkzZTU2ZWQ4ODQxYjhhNzU2ODJlMWJiMjk1ZDcyMjQwNWI%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=vk-yrHOGnMCEOhSDb7DHxb1eLxZIPLky-Uz8d-TzTxPzeUIO%7EwUsN2qQw-2UqoQ4KMHOgjnB0YOzNW2YmPdvSVVya1Lo5T4lNrbi%7EonlVZa3VFFpPDRBYrdyjZLkCh81SJrxLxxT55bhmIMGEHU7CzKvWRrOQlDaQOE8gl2r7u6wrzulBy3F2H7UJPLm8mAaw5rq3bmYnqMxObdhGmzn3Z0rPzkFg8vLIOWfvatnjHi-F61K-B8NdGgaIGh0Kxu5bP98YpicMmaJ0kAEyx5xOV3edMbzO9CKL%7EyGlb%7Edv%7E8BGcptxr5oMhbj0yYzT9qkqdlKLitmjyXceVHEC7exhA__&Key-Pair-Id=K3RPWS32NSSJCE: [SSL: DECRYPTION_FAILED_OR_BAD_RECORD_MAC] decryption failed or bad record mac (_ssl.c:2648)\n",
      "Trying to resume download...\n",
      "Error while downloading from https://cdn-lfs.hf.co/repos/fb/cd/fbcdc88e492959e3ee2515f497fb45cb5f217f9455c204bdf4e0b400c90d0c23/32aa17d6e17b43500f531d5f6dc9bc93e56ed8841b8a75682e1bb295d722405b?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model.safetensors%3B+filename%3D%22model.safetensors%22%3B&Expires=1753078349&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MzA3ODM0OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy9mYi9jZC9mYmNkYzg4ZTQ5Mjk1OWUzZWUyNTE1ZjQ5N2ZiNDVjYjVmMjE3Zjk0NTVjMjA0YmRmNGUwYjQwMGM5MGQwYzIzLzMyYWExN2Q2ZTE3YjQzNTAwZjUzMWQ1ZjZkYzliYzkzZTU2ZWQ4ODQxYjhhNzU2ODJlMWJiMjk1ZDcyMjQwNWI%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=vk-yrHOGnMCEOhSDb7DHxb1eLxZIPLky-Uz8d-TzTxPzeUIO%7EwUsN2qQw-2UqoQ4KMHOgjnB0YOzNW2YmPdvSVVya1Lo5T4lNrbi%7EonlVZa3VFFpPDRBYrdyjZLkCh81SJrxLxxT55bhmIMGEHU7CzKvWRrOQlDaQOE8gl2r7u6wrzulBy3F2H7UJPLm8mAaw5rq3bmYnqMxObdhGmzn3Z0rPzkFg8vLIOWfvatnjHi-F61K-B8NdGgaIGh0Kxu5bP98YpicMmaJ0kAEyx5xOV3edMbzO9CKL%7EyGlb%7Edv%7E8BGcptxr5oMhbj0yYzT9qkqdlKLitmjyXceVHEC7exhA__&Key-Pair-Id=K3RPWS32NSSJCE: [SSL: DECRYPTION_FAILED_OR_BAD_RECORD_MAC] decryption failed or bad record mac (_ssl.c:2648)\n",
      "Trying to resume download...\n",
      "Error while downloading from https://cdn-lfs.hf.co/repos/fb/cd/fbcdc88e492959e3ee2515f497fb45cb5f217f9455c204bdf4e0b400c90d0c23/32aa17d6e17b43500f531d5f6dc9bc93e56ed8841b8a75682e1bb295d722405b?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model.safetensors%3B+filename%3D%22model.safetensors%22%3B&Expires=1753078349&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MzA3ODM0OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy9mYi9jZC9mYmNkYzg4ZTQ5Mjk1OWUzZWUyNTE1ZjQ5N2ZiNDVjYjVmMjE3Zjk0NTVjMjA0YmRmNGUwYjQwMGM5MGQwYzIzLzMyYWExN2Q2ZTE3YjQzNTAwZjUzMWQ1ZjZkYzliYzkzZTU2ZWQ4ODQxYjhhNzU2ODJlMWJiMjk1ZDcyMjQwNWI%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=vk-yrHOGnMCEOhSDb7DHxb1eLxZIPLky-Uz8d-TzTxPzeUIO%7EwUsN2qQw-2UqoQ4KMHOgjnB0YOzNW2YmPdvSVVya1Lo5T4lNrbi%7EonlVZa3VFFpPDRBYrdyjZLkCh81SJrxLxxT55bhmIMGEHU7CzKvWRrOQlDaQOE8gl2r7u6wrzulBy3F2H7UJPLm8mAaw5rq3bmYnqMxObdhGmzn3Z0rPzkFg8vLIOWfvatnjHi-F61K-B8NdGgaIGh0Kxu5bP98YpicMmaJ0kAEyx5xOV3edMbzO9CKL%7EyGlb%7Edv%7E8BGcptxr5oMhbj0yYzT9qkqdlKLitmjyXceVHEC7exhA__&Key-Pair-Id=K3RPWS32NSSJCE: [SSL: DECRYPTION_FAILED_OR_BAD_RECORD_MAC] decryption failed or bad record mac (_ssl.c:2648)\n",
      "Trying to resume download...\n",
      "Error while downloading from https://cdn-lfs.hf.co/repos/fb/cd/fbcdc88e492959e3ee2515f497fb45cb5f217f9455c204bdf4e0b400c90d0c23/32aa17d6e17b43500f531d5f6dc9bc93e56ed8841b8a75682e1bb295d722405b?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model.safetensors%3B+filename%3D%22model.safetensors%22%3B&Expires=1753078349&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MzA3ODM0OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy9mYi9jZC9mYmNkYzg4ZTQ5Mjk1OWUzZWUyNTE1ZjQ5N2ZiNDVjYjVmMjE3Zjk0NTVjMjA0YmRmNGUwYjQwMGM5MGQwYzIzLzMyYWExN2Q2ZTE3YjQzNTAwZjUzMWQ1ZjZkYzliYzkzZTU2ZWQ4ODQxYjhhNzU2ODJlMWJiMjk1ZDcyMjQwNWI%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=vk-yrHOGnMCEOhSDb7DHxb1eLxZIPLky-Uz8d-TzTxPzeUIO%7EwUsN2qQw-2UqoQ4KMHOgjnB0YOzNW2YmPdvSVVya1Lo5T4lNrbi%7EonlVZa3VFFpPDRBYrdyjZLkCh81SJrxLxxT55bhmIMGEHU7CzKvWRrOQlDaQOE8gl2r7u6wrzulBy3F2H7UJPLm8mAaw5rq3bmYnqMxObdhGmzn3Z0rPzkFg8vLIOWfvatnjHi-F61K-B8NdGgaIGh0Kxu5bP98YpicMmaJ0kAEyx5xOV3edMbzO9CKL%7EyGlb%7Edv%7E8BGcptxr5oMhbj0yYzT9qkqdlKLitmjyXceVHEC7exhA__&Key-Pair-Id=K3RPWS32NSSJCE: [SSL: DECRYPTION_FAILED_OR_BAD_RECORD_MAC] decryption failed or bad record mac (_ssl.c:2648)\n",
      "Trying to resume download...\n",
      "C:\\Users\\sayfp\\AppData\\Local\\Temp\\ipykernel_13672\\3773653430.py:210: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=USE_MIXED_PRECISION)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.9812 Acc: 0.6644 | Val Loss: 0.5653 Acc: 0.8067\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 266\u001b[0m\n\u001b[0;32m    264\u001b[0m train_loader, val_loader, test_loader, class_labels \u001b[38;5;241m=\u001b[39m get_data_loaders()\n\u001b[0;32m    265\u001b[0m model \u001b[38;5;241m=\u001b[39m HybridCNNViT(num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(class_labels))\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m--> 266\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;66;03m# Save final model\u001b[39;00m\n\u001b[0;32m    269\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(SAVE_DIR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_hybrid_model.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \n",
      "Cell \u001b[1;32mIn[1], line 226\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, device, train_loader, val_loader, num_epochs)\u001b[0m\n\u001b[0;32m    224\u001b[0m     scaler\u001b[38;5;241m.\u001b[39mstep(optimizer)\n\u001b[0;32m    225\u001b[0m     scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m--> 226\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    227\u001b[0m     running_corrects \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(preds \u001b[38;5;241m==\u001b[39m labels\u001b[38;5;241m.\u001b[39mdata)\n\u001b[0;32m    228\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader\u001b[38;5;241m.\u001b[39mdataset)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train Hybrid CNN+ViT Model for Wheat Disease Detection\n",
    "=====================================================\n",
    "\n",
    "This script trains a hybrid model that combines ConvNeXt (CNN) and ViT (Vision Transformer) with attention-based feature fusion, matching the workflow of train_convnext.ipynb.\n",
    "\n",
    "- Uses torchvision ConvNeXt and timm ViT\n",
    "- Attention-based fusion of CNN and ViT features\n",
    "- Saves best model to saved_models_and_data/best_hybrid_model.pth\n",
    "\n",
    "How to use:\n",
    "-----------\n",
    "1. Ensure your dataset is split into train/val/test in dataset_split/ (see README for format)\n",
    "2. Run: python train_hybrid_cnn_vit.py\n",
    "\n",
    "Author: [Your Name]\n",
    "Date: [Date]\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "try:\n",
    "    import timm\n",
    "except ImportError:\n",
    "    import sys\n",
    "    os.system(f\"{sys.executable} -m pip install timm\")\n",
    "    import timm\n",
    "from timm import create_model\n",
    "\n",
    "from torch.utils.data import WeightedRandomSampler, Subset\n",
    "import shutil\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration\n",
    "# -----------------------------\n",
    "DATASET_DIR = 'dataset'\n",
    "SAVE_DIR = 'saved_models_and_data'\n",
    "SPLIT_OUTPUT_DIR = 'dataset_split'\n",
    "IMAGE_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 1e-4\n",
    "EARLY_STOPPING_PATIENCE = 5\n",
    "USE_MIXED_PRECISION = True if torch.cuda.is_available() else False\n",
    "\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(SPLIT_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Transformations\n",
    "# -----------------------------\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(IMAGE_SIZE),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(45),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(IMAGE_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# -----------------------------\n",
    "# Custom Dataset\n",
    "# -----------------------------\n",
    "class WheatDiseaseDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        # Only include subdirectories as classes\n",
    "        self.classes = sorted([d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))])\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "        self.samples = []\n",
    "        for target_class in self.classes:\n",
    "            class_dir = os.path.join(root_dir, target_class)\n",
    "            for img_file in os.listdir(class_dir):\n",
    "                path = os.path.join(class_dir, img_file)\n",
    "                self.samples.append((path, self.class_to_idx[target_class]))\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        path, target = self.samples[idx]\n",
    "        try:\n",
    "            image = Image.open(path).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, target\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors du chargement de {path}: {e}\")\n",
    "            return self.__getitem__((idx + 1) % len(self))\n",
    "\n",
    "# -----------------------------\n",
    "# Hybrid Model Definition\n",
    "# -----------------------------\n",
    "class AttentionFusion(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Sequential(\n",
    "            nn.Linear(in_dim, in_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_dim, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # x: (batch, features)\n",
    "        attn_weights = torch.softmax(self.attn(x), dim=1)\n",
    "        return (x * attn_weights).sum(dim=1)\n",
    "\n",
    "class HybridCNNViT(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        # CNN branch (ConvNeXt)\n",
    "        self.cnn = models.convnext_base(pretrained=True)\n",
    "        cnn_out = self.cnn.classifier[2].in_features\n",
    "        self.cnn.classifier = nn.Identity()\n",
    "        # ViT branch (from timm)\n",
    "        self.vit = create_model('vit_base_patch16_224', pretrained=True)\n",
    "        vit_out = self.vit.head.in_features\n",
    "        self.vit.head = nn.Identity()\n",
    "        # Attention-based fusion\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(cnn_out + vit_out, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        cnn_feat = self.cnn.features(x)  # (batch, C, H, W)\n",
    "        cnn_feat = cnn_feat.mean(dim=[2, 3])  # Global average pool to (batch, C)\n",
    "        vit_feat = self.vit(x)  # (batch, features)\n",
    "        fused = torch.cat([cnn_feat, vit_feat], dim=1)\n",
    "        out = self.fusion(fused)\n",
    "        return out\n",
    "\n",
    "# -----------------------------\n",
    "# Data Loaders\n",
    "# -----------------------------\n",
    "def get_data_loaders():\n",
    "    split_dirs = [os.path.join(SPLIT_OUTPUT_DIR, split) for split in ['train', 'val', 'test']]\n",
    "    split_exists = all(os.path.isdir(d) and len(os.listdir(d)) > 0 for d in split_dirs)\n",
    "    if split_exists:\n",
    "        print('Found existing split dataset. Loading splits...')\n",
    "        train_dataset = WheatDiseaseDataset(os.path.join(SPLIT_OUTPUT_DIR, 'train'), transform=train_transform)\n",
    "        val_dataset = WheatDiseaseDataset(os.path.join(SPLIT_OUTPUT_DIR, 'val'), transform=test_transform)\n",
    "        test_dataset = WheatDiseaseDataset(os.path.join(SPLIT_OUTPUT_DIR, 'test'), transform=test_transform)\n",
    "    else:\n",
    "        print('No split dataset found. Splitting and saving images...')\n",
    "        full_dataset = WheatDiseaseDataset(DATASET_DIR, transform=train_transform)\n",
    "        generator = torch.Generator().manual_seed(42)\n",
    "        indices = torch.randperm(len(full_dataset), generator=generator).tolist()\n",
    "        train_size = int(0.7 * len(full_dataset))\n",
    "        val_size = int(0.15 * len(full_dataset))\n",
    "        test_size = len(full_dataset) - train_size - val_size\n",
    "        train_indices = indices[:train_size]\n",
    "        val_indices = indices[train_size:train_size + val_size]\n",
    "        test_indices = indices[train_size + val_size:]\n",
    "        train_data = Subset(full_dataset, train_indices)\n",
    "        val_data = Subset(full_dataset, val_indices)\n",
    "        test_data = Subset(full_dataset, test_indices)\n",
    "        def save_split_images(dataset, indices, split_name):\n",
    "            print(f\"Saving images for split: {split_name}\")\n",
    "            for idx in indices:\n",
    "                path, label_idx = dataset.dataset.samples[idx]  # dataset is a Subset\n",
    "                class_name = dataset.dataset.classes[label_idx]\n",
    "                filename = os.path.basename(path)\n",
    "                dest_dir = os.path.join(SPLIT_OUTPUT_DIR, split_name, class_name)\n",
    "                os.makedirs(dest_dir, exist_ok=True)\n",
    "                dest_path = os.path.join(dest_dir, filename)\n",
    "                shutil.copyfile(path, dest_path)\n",
    "        save_split_images(train_data, train_indices, 'train')\n",
    "        save_split_images(val_data, val_indices, 'val')\n",
    "        save_split_images(test_data, test_indices, 'test')\n",
    "        print('Image splits saved.')\n",
    "        train_dataset = WheatDiseaseDataset(os.path.join(SPLIT_OUTPUT_DIR, 'train'), transform=train_transform)\n",
    "        val_dataset = WheatDiseaseDataset(os.path.join(SPLIT_OUTPUT_DIR, 'val'), transform=test_transform)\n",
    "        test_dataset = WheatDiseaseDataset(os.path.join(SPLIT_OUTPUT_DIR, 'test'), transform=test_transform)\n",
    "    print('Calculating class weights for balanced sampling...')\n",
    "    targets = [s[1] for s in train_dataset.samples]\n",
    "    class_counts = np.bincount(targets)\n",
    "    class_weights = 1. / class_counts\n",
    "    sample_weights = [class_weights[t] for t in targets]\n",
    "    sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=sampler)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    print('Data loaders are ready.')\n",
    "    return train_loader, val_loader, test_loader, train_dataset.classes\n",
    "\n",
    "# -----------------------------\n",
    "# Training Loop\n",
    "# -----------------------------\n",
    "def train_model(model, device, train_loader, val_loader, num_epochs=EPOCHS):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2)\n",
    "    best_acc = 0.0\n",
    "    no_improvement_epochs = 0\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=USE_MIXED_PRECISION)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            autocast_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "            with torch.amp.autocast(autocast_device, enabled=USE_MIXED_PRECISION):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc = running_corrects.float() / len(train_loader.dataset)\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_corrects = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                with torch.amp.autocast(autocast_device, enabled=USE_MIXED_PRECISION):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                val_corrects += torch.sum(preds == labels.data)\n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_acc = val_corrects.float() / len(val_loader.dataset)\n",
    "        scheduler.step(val_loss)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} | Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}\")\n",
    "        # Early Stopping\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            torch.save(model.state_dict(), os.path.join(SAVE_DIR, \"best_hybrid_model.pth\"))\n",
    "            no_improvement_epochs = 0\n",
    "        else:\n",
    "            no_improvement_epochs += 1\n",
    "        if no_improvement_epochs >= EARLY_STOPPING_PATIENCE:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "    print(\"Training complete.\")\n",
    "    return model\n",
    "\n",
    "# -----------------------------\n",
    "# Main\n",
    "# -----------------------------\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_loader, val_loader, test_loader, class_labels = get_data_loaders()\n",
    "model = HybridCNNViT(num_classes=len(class_labels)).to(device)\n",
    "model = train_model(model, device, train_loader, val_loader)\n",
    "\n",
    "# Save final model\n",
    "torch.save(model.state_dict(), os.path.join(SAVE_DIR, \"final_hybrid_model.pth\")) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
