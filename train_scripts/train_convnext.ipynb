{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356370f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, WeightedRandomSampler\n",
    "from torchvision import transforms, models\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shutil\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration\n",
    "# -----------------------------\n",
    "DATASET_DIR = '../dataset'\n",
    "SAVE_DIR = '../saved_models_and_data'\n",
    "SPLIT_OUTPUT_DIR = '../dataset_split'\n",
    "IMAGE_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "TEST_SIZE = 0.15\n",
    "VAL_SIZE = 0.15\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 1e-4\n",
    "EARLY_STOPPING_PATIENCE = 5\n",
    "USE_MIXED_PRECISION = True if torch.cuda.is_available() else False\n",
    "\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(SPLIT_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Training Function with Optimizations\n",
    "# -----------------------------\n",
    "def train_model(model, device, train_loader, val_loader, num_epochs=EPOCHS):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2)\n",
    "    best_acc = 0.0\n",
    "    no_improvement_epochs = 0\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=USE_MIXED_PRECISION)\n",
    "    train_log = []\n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cudnn.benchmark = True  # optimize for speed\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            with torch.cuda.amp.autocast(enabled=USE_MIXED_PRECISION):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc = running_corrects.double() / len(train_loader.dataset)\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_corrects = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "                with torch.cuda.amp.autocast(enabled=USE_MIXED_PRECISION):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                val_corrects += torch.sum(preds == labels.data)\n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_acc = val_corrects.double() / len(val_loader.dataset)\n",
    "        scheduler.step(val_loss)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} | Val Loss: {val_loss:.4f} Acc: {val_acc:.4f} | LR: {current_lr:.6f} | Time: {time.time()-start_time:.1f}s\")\n",
    "        train_log.append({'epoch': epoch+1, 'train_loss': epoch_loss, 'train_acc': epoch_acc.item(), 'val_loss': val_loss, 'val_acc': val_acc.item(), 'lr': current_lr})\n",
    "        # Early Stopping\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            torch.save(model.state_dict(), os.path.join(SAVE_DIR, \"best_convnext_model.pth\"))\n",
    "            no_improvement_epochs = 0\n",
    "        else:\n",
    "            no_improvement_epochs += 1\n",
    "        if no_improvement_epochs >= EARLY_STOPPING_PATIENCE:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "    print(\"Entraînement terminé.\")\n",
    "    return model, train_log\n",
    "\n",
    "\n",
    "print('Setting up image transformations for training and testing...')\n",
    "# -----------------------------\n",
    "# Transformations\n",
    "# -----------------------------\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(IMAGE_SIZE),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(45),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(IMAGE_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "print('Image transformations are ready.')\n",
    "\n",
    "print('Defining custom dataset class for wheat disease images...')\n",
    "# -----------------------------\n",
    "# Custom Dataset\n",
    "# -----------------------------\n",
    "class WheatDiseaseDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = sorted(os.listdir(root_dir))\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "        self.samples = []\n",
    "        for target_class in self.classes:\n",
    "            class_dir = os.path.join(root_dir, target_class)\n",
    "            for img_file in os.listdir(class_dir):\n",
    "                path = os.path.join(class_dir, img_file)\n",
    "                self.samples.append((path, self.class_to_idx[target_class]))\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        path, target = self.samples[idx]\n",
    "        try:\n",
    "            image = Image.open(path).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, target\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors du chargement de {path}: {e}\")\n",
    "            return self.__getitem__((idx + 1) % len(self))\n",
    "print('Custom dataset class defined.')\n",
    "\n",
    "print('Preparing data loaders and splitting dataset if needed...')\n",
    "# -----------------------------\n",
    "# Load and Split Dataset\n",
    "# -----------------------------\n",
    "def get_data_loaders():\n",
    "    split_dirs = [os.path.join(SPLIT_OUTPUT_DIR, split) for split in ['train', 'val', 'test']]\n",
    "    split_exists = all(os.path.isdir(d) and len(os.listdir(d)) > 0 for d in split_dirs)\n",
    "    if split_exists:\n",
    "        print('Found existing split dataset. Loading splits...')\n",
    "        train_dataset = WheatDiseaseDataset(os.path.join(SPLIT_OUTPUT_DIR, 'train'), transform=train_transform)\n",
    "        val_dataset = WheatDiseaseDataset(os.path.join(SPLIT_OUTPUT_DIR, 'val'), transform=test_transform)\n",
    "        test_dataset = WheatDiseaseDataset(os.path.join(SPLIT_OUTPUT_DIR, 'test'), transform=test_transform)\n",
    "    else:\n",
    "        print('No split dataset found. Splitting and saving images...')\n",
    "        full_dataset = WheatDiseaseDataset(DATASET_DIR, transform=train_transform)\n",
    "        generator = torch.Generator().manual_seed(42)\n",
    "        indices = torch.randperm(len(full_dataset), generator=generator).tolist()\n",
    "        train_size = int((1 - TEST_SIZE - VAL_SIZE) * len(full_dataset))\n",
    "        val_size = int(VAL_SIZE * len(full_dataset))\n",
    "        test_size = len(full_dataset) - train_size - val_size\n",
    "        train_indices = indices[:train_size]\n",
    "        val_indices = indices[train_size:train_size + val_size]\n",
    "        test_indices = indices[train_size + val_size:]\n",
    "        train_data = Subset(full_dataset, train_indices)\n",
    "        val_data = Subset(full_dataset, val_indices)\n",
    "        test_data = Subset(full_dataset, test_indices)\n",
    "        def save_split_images(dataset, indices, split_name):\n",
    "            print(f\"Saving images for split: {split_name}\")\n",
    "            for idx in indices:\n",
    "                path, label_idx = dataset.dataset.samples[idx]  # dataset is a Subset\n",
    "                class_name = dataset.dataset.classes[label_idx]\n",
    "                filename = os.path.basename(path)\n",
    "                dest_dir = os.path.join(SPLIT_OUTPUT_DIR, split_name, class_name)\n",
    "                os.makedirs(dest_dir, exist_ok=True)\n",
    "                dest_path = os.path.join(dest_dir, filename)\n",
    "                shutil.copyfile(path, dest_path)\n",
    "        save_split_images(train_data, train_indices, 'train')\n",
    "        save_split_images(val_data, val_indices, 'val')\n",
    "        save_split_images(test_data, test_indices, 'test')\n",
    "        print('Image splits saved.')\n",
    "        train_dataset = WheatDiseaseDataset(os.path.join(SPLIT_OUTPUT_DIR, 'train'), transform=train_transform)\n",
    "        val_dataset = WheatDiseaseDataset(os.path.join(SPLIT_OUTPUT_DIR, 'val'), transform=test_transform)\n",
    "        test_dataset = WheatDiseaseDataset(os.path.join(SPLIT_OUTPUT_DIR, 'test'), transform=test_transform)\n",
    "    print('Calculating class weights for balanced sampling...')\n",
    "    targets = [s[1] for s in train_dataset.samples]\n",
    "    class_counts = np.bincount(targets)\n",
    "    class_weights = 1. / class_counts\n",
    "    sample_weights = [class_weights[t] for t in targets]\n",
    "    sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=sampler, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    print('Data loaders are ready.')\n",
    "    return train_loader, val_loader, test_loader, train_dataset.classes\n",
    "\n",
    "# -----------------------------\n",
    "# Model Loader Function & Optimizations\n",
    "# -----------------------------\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "def load_model(num_classes):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    # Load ConvNeXt base model\n",
    "    model = models.convnext_base(pretrained=True)\n",
    "    # Replace classifier head\n",
    "    in_features = model.classifier[2].in_features\n",
    "    model.classifier[2] = nn.Linear(in_features, num_classes)\n",
    "    model = model.to(device)\n",
    "    return model, device\n",
    "\n",
    "# Optionally, print CUDA info for debugging\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "    print(f\"CUDA device name: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Main Execution (Notebook Style)\n",
    "# -----------------------------\n",
    "print('Loading data...')\n",
    "train_loader, val_loader, test_loader, class_labels = get_data_loaders()\n",
    "print('Data loaded. Classes:', class_labels)\n",
    "print('Initializing model...')\n",
    "model, device = load_model(len(class_labels))\n",
    "print('Model initialized. Starting training...')\n",
    "model, train_log = train_model(model, device, train_loader, val_loader)\n",
    "print('Training complete. Evaluating on test set...')\n",
    "# Evaluation on Test Set\n",
    "model.load_state_dict(torch.load(os.path.join(SAVE_DIR, \"best_convnext_model.pth\"), map_location=device))\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "print('Test set predictions complete. Generating confusion matrix...')\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.xlabel(\"Prédit\")\n",
    "plt.ylabel(\"Réel\")\n",
    "plt.title(\"Matrice de Confusion (ConvNeXt)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"\\nRapport de classification:\")\n",
    "print(classification_report(y_true, y_pred, target_names=class_labels, digits=4))\n",
    "torch.save(model.state_dict(), os.path.join(SAVE_DIR, \"wheat_disease_convnext_model.pth\"))\n",
    "print('Model saved to', os.path.join(SAVE_DIR, \"wheat_disease_convnext_model.pth\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
