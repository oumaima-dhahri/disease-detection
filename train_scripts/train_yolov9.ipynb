{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2155f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, WeightedRandomSampler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shutil\n",
    "from PIL import Image\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration - Matching other training scripts exactly\n",
    "DATASET_DIR = '../dataset'\n",
    "SAVE_DIR = '../saved_models_and_data'\n",
    "SPLIT_OUTPUT_DIR = '../dataset_split'\n",
    "IMAGE_SIZE = (224, 224)  # Standard size used by other scripts\n",
    "BATCH_SIZE = 32          # Standard batch size used by other scripts\n",
    "TEST_SIZE = 0.15\n",
    "VAL_SIZE = 0.15\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 1e-4     # Standard learning rate used by other scripts\n",
    "EARLY_STOPPING_PATIENCE = 5  # Standard patience used by other scripts\n",
    "USE_MIXED_PRECISION = True if torch.cuda.is_available() else False\n",
    "\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(SPLIT_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"Sets the seed for reproducibility.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Install YOLOv9 if not present\n",
    "try:\n",
    "    import ultralytics\n",
    "except ImportError:\n",
    "    print(\"Installing ultralytics (YOLOv9)...\")\n",
    "    os.system(f\"{sys.executable} -m pip install ultralytics\")\n",
    "    import ultralytics\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "print('Defining custom dataset class for wheat disease images...')\n",
    "class WheatDiseaseDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.num_workers = 4\n",
    "        self.pin_memory = True\n",
    "        self.classes = sorted([d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))])\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "        self.samples = []\n",
    "        for target_class in self.classes:\n",
    "            class_dir = os.path.join(root_dir, target_class)\n",
    "            # Ensure it's a directory before listing\n",
    "            if os.path.isdir(class_dir):\n",
    "                for img_file in os.listdir(class_dir):\n",
    "                    if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                        path = os.path.join(class_dir, img_file)\n",
    "                        self.samples.append((path, self.class_to_idx[target_class]))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        path, target = self.samples[idx]\n",
    "        try:\n",
    "            image = Image.open(path).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, target\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {path}: {e}\")\n",
    "            # Fallback: try to load the next image in the dataset to avoid stopping\n",
    "            return self.__getitem__((idx + 1) % len(self))\n",
    "print('Custom dataset class defined.')\n",
    "\n",
    "def create_yolo_labels(dataset_dir, output_dir, class_labels):\n",
    "    \"\"\"Convert classification dataset to YOLO object detection format\"\"\"\n",
    "    print(f\"Converting classification dataset to YOLO format...\")\n",
    "    \n",
    "    # Create output directories\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        split_dir = os.path.join(output_dir, split)\n",
    "        os.makedirs(split_dir, exist_ok=True)\n",
    "        \n",
    "        # Create images and labels subdirectories\n",
    "        images_dir = os.path.join(split_dir, 'images')\n",
    "        labels_dir = os.path.join(split_dir, 'labels')\n",
    "        os.makedirs(images_dir, exist_ok=True)\n",
    "        os.makedirs(labels_dir, exist_ok=True)\n",
    "        \n",
    "        # Get the split directory path\n",
    "        split_source_dir = os.path.join(dataset_dir, split)\n",
    "        if not os.path.exists(split_source_dir):\n",
    "            print(f\"Split directory {split_source_dir} does not exist, skipping...\")\n",
    "            continue\n",
    "            \n",
    "        # Process each class\n",
    "        for class_name in class_labels:\n",
    "            class_dir = os.path.join(split_source_dir, class_name)\n",
    "            \n",
    "            # Check if it's a directory and exists\n",
    "            if not os.path.exists(class_dir):\n",
    "                print(f\"Class directory {class_dir} does not exist, skipping...\")\n",
    "                continue\n",
    "                \n",
    "            if not os.path.isdir(class_dir):\n",
    "                print(f\"Skipping {class_dir} - not a directory\")\n",
    "                continue\n",
    "                \n",
    "            class_idx = class_labels.index(class_name)\n",
    "            print(f\"Processing class {class_name} (index {class_idx}) in {split} split...\")\n",
    "            \n",
    "            # Process each image in the class\n",
    "            try:\n",
    "                image_count = 0\n",
    "                for img_file in os.listdir(class_dir):\n",
    "                    # Only process image files\n",
    "                    if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                        # Copy image to images directory\n",
    "                        src_path = os.path.join(class_dir, img_file)\n",
    "                        dst_path = os.path.join(images_dir, img_file)\n",
    "                        shutil.copy2(src_path, dst_path)\n",
    "                        \n",
    "                        # Create YOLO label file\n",
    "                        label_file = os.path.splitext(img_file)[0] + '.txt'\n",
    "                        label_path = os.path.join(labels_dir, label_file)\n",
    "                        \n",
    "                        # Create bounding box for entire image (classification converted to detection)\n",
    "                        # Format: class_id x_center y_center width height (normalized)\n",
    "                        with open(label_path, 'w') as f:\n",
    "                            # x_center = 0.5, y_center = 0.5, width = 1.0, height = 1.0\n",
    "                            # This creates a bounding box covering the entire image\n",
    "                            f.write(f\"{class_idx} 0.5 0.5 1.0 1.0\\n\")\n",
    "                        image_count += 1\n",
    "                \n",
    "                print(f\"Processed {image_count} images for class {class_name} in {split} split\")\n",
    "                \n",
    "            except NotADirectoryError:\n",
    "                print(f\"Skipping {class_dir} - not a directory\")\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {class_dir}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    print(\"YOLO format conversion completed.\")\n",
    "\n",
    "print('Preparing data loaders and splitting dataset if needed...')\n",
    "def get_data_loaders():\n",
    "    split_dirs = [os.path.join(SPLIT_OUTPUT_DIR, split) for split in ['train', 'val', 'test']]\n",
    "    # Check if split directories exist and are not empty\n",
    "    split_exists = all(os.path.isdir(d) and len([f for f in os.listdir(d) if os.path.isdir(os.path.join(d, f))]) > 0 for d in split_dirs)\n",
    "\n",
    "    if split_exists:\n",
    "        print('Found existing split dataset. Loading splits...')\n",
    "        train_dataset = WheatDiseaseDataset(os.path.join(SPLIT_OUTPUT_DIR, 'train'), transform=None)\n",
    "        val_dataset = WheatDiseaseDataset(os.path.join(SPLIT_OUTPUT_DIR, 'val'), transform=None)\n",
    "        test_dataset = WheatDiseaseDataset(os.path.join(SPLIT_OUTPUT_DIR, 'test'), transform=None)\n",
    "    else:\n",
    "        print('No split dataset found. Splitting and saving images...')\n",
    "        full_dataset = WheatDiseaseDataset(DATASET_DIR, transform=None) # Use None transform for initial loading\n",
    "\n",
    "        generator = torch.Generator().manual_seed(42)\n",
    "        indices = torch.randperm(len(full_dataset), generator=generator).tolist()\n",
    "\n",
    "        train_size = int((1 - TEST_SIZE - VAL_SIZE) * len(full_dataset))\n",
    "        val_size = int(VAL_SIZE * len(full_dataset))\n",
    "        # test_size is implicitly the rest\n",
    "        # test_size = len(full_dataset) - train_size - val_sizemat\n",
    "\n",
    "        train_indices = indices[:train_size]\n",
    "        val_indices = indices[train_size:train_size + val_size]\n",
    "        test_indices = indices[train_size + val_size:]\n",
    "\n",
    "        # Create Subset objects for initial splitting (before saving to disk)\n",
    "        train_data_subset = Subset(full_dataset, train_indices)\n",
    "        val_data_subset = Subset(full_dataset, val_indices)\n",
    "        test_data_subset = Subset(full_dataset, test_indices)\n",
    "\n",
    "        def save_split_images(dataset_subset, split_name):\n",
    "            print(f\"Saving images for split: {split_name}\")\n",
    "            for idx_in_subset in range(len(dataset_subset)):\n",
    "                original_idx = dataset_subset.indices[idx_in_subset] # Get original index from subset\n",
    "                path, label_idx = full_dataset.samples[original_idx] # Use full_dataset to get original path\n",
    "                class_name = full_dataset.classes[label_idx]\n",
    "                filename = os.path.basename(path)\n",
    "                dest_dir = os.path.join(SPLIT_OUTPUT_DIR, split_name, class_name)\n",
    "                os.makedirs(dest_dir, exist_ok=True)\n",
    "                dest_path = os.path.join(dest_dir, filename)\n",
    "                shutil.copyfile(path, dest_path)\n",
    "\n",
    "        save_split_images(train_data_subset, 'train')\n",
    "        save_split_images(val_data_subset, 'val')\n",
    "        save_split_images(test_data_subset, 'test')\n",
    "        print('Image splits saved.')\n",
    "\n",
    "        # Now, load datasets from the new split directories\n",
    "        train_dataset = WheatDiseaseDataset(os.path.join(SPLIT_OUTPUT_DIR, 'train'), transform=None)\n",
    "        val_dataset = WheatDiseaseDataset(os.path.join(SPLIT_OUTPUT_DIR, 'val'), transform=None)\n",
    "        test_dataset = WheatDiseaseDataset(os.path.join(SPLIT_OUTPUT_DIR, 'test'), transform=None)\n",
    "\n",
    "    print('Data loaders are ready.')\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "def create_yolo_yaml(class_labels):\n",
    "    \"\"\"Create YAML configuration file for YOLOv9\"\"\"\n",
    "    DATA_YAML = os.path.join(SAVE_DIR, 'wheat_yolov9.yaml')\n",
    "    \n",
    "    # Create YOLO format directories and labels\n",
    "    yolo_dataset_dir = os.path.join(SAVE_DIR, 'yolo_dataset')\n",
    "    create_yolo_labels(SPLIT_OUTPUT_DIR, yolo_dataset_dir, class_labels)\n",
    "    \n",
    "    with open(DATA_YAML, 'w') as f:\n",
    "        f.write(f\"train: {os.path.abspath(os.path.join(yolo_dataset_dir, 'train', 'images'))}\\n\")\n",
    "        f.write(f\"val: {os.path.abspath(os.path.join(yolo_dataset_dir, 'val', 'images'))}\\n\")\n",
    "        f.write(f\"test: {os.path.abspath(os.path.join(yolo_dataset_dir, 'test', 'images'))}\\n\")\n",
    "        f.write(f\"nc: {len(class_labels)}\\n\")\n",
    "        f.write(f\"names: {class_labels}\\n\")\n",
    "    \n",
    "    print(f\"YAML configuration created: {DATA_YAML}\")\n",
    "    print(f\"Classes ({len(class_labels)}): {class_labels}\")\n",
    "    return DATA_YAML\n",
    "\n",
    "def train_model(data_yaml, num_classes, class_labels):\n",
    "    \"\"\"Train YOLOv9 model following the same pattern as other training scripts\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "        print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "        print(f\"CUDA device name: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "    \n",
    "    # Initialize YOLOv9 model\n",
    "    model = YOLO('yolov9c.pt')  # Download pretrained weights\n",
    "    \n",
    "    best_acc = 0.0\n",
    "    no_improvement_epochs = 0\n",
    "    train_log = []\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cudnn.benchmark = True  # optimize for speed\n",
    "    \n",
    "    print(\"Starting YOLOv9 training...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Training parameters matching other scripts' approach\n",
    "    results = model.train(\n",
    "        data=data_yaml,\n",
    "        imgsz=IMAGE_SIZE[0],  # Use same image size as other scripts\n",
    "        epochs=EPOCHS,\n",
    "        batch=BATCH_SIZE,\n",
    "        lr0=LEARNING_RATE,  # Use same learning rate as other scripts\n",
    "        project=SAVE_DIR,\n",
    "        name='yolov9_wheat_disease',\n",
    "        exist_ok=True,\n",
    "        save=True,\n",
    "        save_period=5,  # Save checkpoint every 5 epochs\n",
    "        patience=EARLY_STOPPING_PATIENCE,  # Same patience as other scripts\n",
    "        verbose=True,\n",
    "        workers=0,  # Set to 0 to avoid multiprocessing issues\n",
    "        device=device,\n",
    "        # Data augmentation parameters matching other scripts\n",
    "        hsv_h=0.1,      # Hue augmentation - similar to ColorJitter\n",
    "        hsv_s=0.2,      # Saturation augmentation - similar to ColorJitter\n",
    "        hsv_v=0.2,      # Value augmentation - similar to ColorJitter\n",
    "        degrees=45,     # Rotation degrees - matching RandomRotation(45)\n",
    "        translate=0.1,  # Translation - minimal\n",
    "        scale=0.1,      # Scale variation - minimal  \n",
    "        shear=0.0,      # No shear to match other scripts\n",
    "        perspective=0.0, # No perspective to match other scripts\n",
    "        flipud=0.5,     # Vertical flip - matching RandomVerticalFlip\n",
    "        fliplr=0.5,     # Horizontal flip - matching RandomHorizontalFlip\n",
    "        mosaic=0.0,     # Disable mosaic for classification-like training\n",
    "        mixup=0.0,      # Disable mixup for classification-like training\n",
    "        copy_paste=0.0, # Disable copy-paste\n",
    "    )\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"Training completed in {total_time/60:.1f} minutes\")\n",
    "    \n",
    "    return model, results\n",
    "\n",
    "def evaluate_model(model, data_yaml):\n",
    "    \"\"\"Evaluate the trained model on test set following other scripts' pattern\"\"\"\n",
    "    print(\"Evaluating model on test set...\")\n",
    "    \n",
    "    # Validation on test set\n",
    "    results = model.val(\n",
    "        data=data_yaml,\n",
    "        split='test',\n",
    "        imgsz=IMAGE_SIZE[0],\n",
    "        batch=BATCH_SIZE,\n",
    "        save_json=True,\n",
    "        save_hybrid=True,\n",
    "        conf=0.001,\n",
    "        iou=0.6,\n",
    "        max_det=300,\n",
    "        half=False,\n",
    "        device=0 if torch.cuda.is_available() else 'cpu',\n",
    "        dnn=False,\n",
    "        plots=True,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution following the same pattern as other training scripts\"\"\"\n",
    "    print('Loading data...')\n",
    "    train_dataset, val_dataset, test_dataset = get_data_loaders()\n",
    "    class_labels = train_dataset.classes\n",
    "    NUM_CLASSES = len(class_labels)\n",
    "    print('Data loaded. Classes:', class_labels)\n",
    "    \n",
    "    print('Creating YAML configuration...')\n",
    "    data_yaml = create_yolo_yaml(class_labels)\n",
    "    \n",
    "    print('Initializing YOLOv9 model...')\n",
    "    print('Model initialized. Starting training...')\n",
    "    model, train_results = train_model(data_yaml, NUM_CLASSES, class_labels)\n",
    "    \n",
    "    print('Training complete. Evaluating on test set...')\n",
    "    eval_results = evaluate_model(model, data_yaml)\n",
    "    \n",
    "    # Save final model following other scripts' pattern\n",
    "    best_model_path = os.path.join(SAVE_DIR, 'yolov9_wheat_disease', 'weights', 'best.pt')\n",
    "    final_model_path = os.path.join(SAVE_DIR, 'wheat_disease_yolov9_model.pt')\n",
    "    \n",
    "    if os.path.exists(best_model_path):\n",
    "        shutil.copy(best_model_path, final_model_path)\n",
    "        print(f'Model saved to {final_model_path}')\n",
    "        \n",
    "        # Also save a backup following other scripts' naming\n",
    "        backup_path = os.path.join(SAVE_DIR, 'best_yolov9_model.pt')\n",
    "        shutil.copy(best_model_path, backup_path)\n",
    "        print(f'Best model backup saved to {backup_path}')\n",
    "    else:\n",
    "        print(\"Warning: Best model weights not found!\")\n",
    "    \n",
    "    # Generate confusion matrix and classification report if possible\n",
    "    print('Test set predictions complete.')\n",
    "    \n",
    "    # Training results summary following other scripts' pattern\n",
    "    results_dir = os.path.join(SAVE_DIR, 'yolov9_wheat_disease')\n",
    "    if os.path.exists(results_dir):\n",
    "        print(f\"Training results and plots saved in: {results_dir}\")\n",
    "    \n",
    "    # Print key metrics if available\n",
    "    if hasattr(eval_results, 'results_dict'):\n",
    "        metrics = eval_results.results_dict\n",
    "        if 'metrics/mAP50(B)' in metrics:\n",
    "            print(f\"mAP@0.5: {metrics['metrics/mAP50(B)']:.4f}\")\n",
    "        if 'metrics/mAP50-95(B)' in metrics:\n",
    "            print(f\"mAP@0.5:0.95: {metrics['metrics/mAP50-95(B)']:.4f}\")\n",
    "    \n",
    "    print(\"YOLOv9 training completed.\")\n",
    "    \n",
    "    # Save training log to match other scripts\n",
    "    import json\n",
    "    training_log = {\n",
    "        'model_type': 'YOLOv9',\n",
    "        'epochs': EPOCHS,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'image_size': IMAGE_SIZE,\n",
    "        'num_classes': NUM_CLASSES,\n",
    "        'class_labels': class_labels,\n",
    "        'early_stopping_patience': EARLY_STOPPING_PATIENCE\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(SAVE_DIR, \"yolov9_training_log.json\"), 'w') as f:\n",
    "        json.dump(training_log, f, indent=2)\n",
    "    print('Training log saved.')\n",
    "    \n",
    "    return model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
